{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69",
   "display_name": "Python 3.7.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 라벨링된 채널 정보 -> 비디오 100개씩 추출 및 라벨링 적용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UCWCOoCApFUtccYENOYeL1uw vidoes OK\n",
      "0 UCWCOoCApFUtccYENOYeL1uw OK\n",
      "UCqa2MPu8bLY1PwVFUpSyVhQ vidoes OK\n",
      "1 UCqa2MPu8bLY1PwVFUpSyVhQ OK\n",
      "UCAo41QETLlwvJ-oqjmEPt8A vidoes OK\n",
      "2 UCAo41QETLlwvJ-oqjmEPt8A OK\n",
      "no more videos\n",
      "3 UCAOtE1V7Ots4DjM8JLlrYgg OK\n",
      "no more videos\n",
      "4 UCoookXUzPciGrEZEXmh4Jjg OK\n",
      "no more videos\n",
      "UCLbyJSPEe2lx3jY-G0CFR3g vidoes OK\n",
      "5 UCLbyJSPEe2lx3jY-G0CFR3g OK\n",
      "no more videos\n",
      "6 UCDCNmuaOXOo25Yn4mbMHhhQ OK\n",
      "UCFuU-5B1eKAWaTeLUu3JuyA vidoes OK\n",
      "7 UCFuU-5B1eKAWaTeLUu3JuyA OK\n",
      "no more videos\n",
      "8 UCcdwLMPsaU2ezNSJU1nFoBQ OK\n",
      "no more videos\n",
      "UCbCmjCuTUZos6Inko4u57UQ vidoes OK\n",
      "9 UCbCmjCuTUZos6Inko4u57UQ OK\n",
      "no more videos\n",
      "10 UCoL0M9swO14BT8u9pTn9MvQ OK\n",
      "no more videos\n",
      "11 UCE_URvMQZgvPqd8rzlFF58A OK\n",
      "no more videos\n",
      "12 UC5PYHgAzJ1wLEidB58SK6Xw OK\n",
      "UCvlE5gTbOvjiolFlEm-c_Ow vidoes OK\n",
      "13 UCvlE5gTbOvjiolFlEm-c_Ow OK\n",
      "no more videos\n",
      "14 UCH2cMLriCDcvb9Uq_2kf17A OK\n",
      "no more videos\n",
      "15 UCUsN5ZwHx2kILm84-jPDeXw OK\n",
      "no more videos\n",
      "16 UCRijo3ddMTht_IHyNSNXpNQ OK\n",
      "UCa6vGFO9ty8v5KZJXQxdhaw vidoes OK\n",
      "17 UCa6vGFO9ty8v5KZJXQxdhaw OK\n",
      "no more videos\n",
      "UCG8rbF3g2AMX70yOd8vqIZg vidoes OK\n",
      "18 UCG8rbF3g2AMX70yOd8vqIZg OK\n",
      "UC3gNmTGu-TTbFPpfSs5kNkg vidoes OK\n",
      "19 UC3gNmTGu-TTbFPpfSs5kNkg OK\n",
      "no more videos\n",
      "UCX6OQ3DkcsbYNE6H8uQQuVA vidoes OK\n",
      "20 UCX6OQ3DkcsbYNE6H8uQQuVA OK\n",
      "no more videos\n",
      "UCqFzWxSCi39LnW1JKFR3efg vidoes OK\n",
      "21 UCqFzWxSCi39LnW1JKFR3efg OK\n",
      "no more videos\n",
      "22 UCp0hYYBW6IMayGgR-WeoCvQ OK\n",
      "no more videos\n",
      "23 UCHnyfMqiRRG1u-2MsSQLbXA OK\n",
      "no more videos\n",
      "24 UCn6BJylSkl-Gd66qepnmHzg OK\n",
      "no more videos\n",
      "UC16niRr50-MSBwiO3YDb3RA vidoes OK\n",
      "25 UC16niRr50-MSBwiO3YDb3RA OK\n",
      "no more videos\n",
      "26 UCqnbDFdCpuN8CMEg0VuEBqA OK\n",
      "no more videos\n",
      "UCpVm7bg6pXKo1Pr6k5kxG9A vidoes OK\n",
      "27 UCpVm7bg6pXKo1Pr6k5kxG9A OK\n",
      "no more videos\n",
      "UCz1hQ68G3XPVYEBoFDgSjcQ vidoes OK\n",
      "28 UCz1hQ68G3XPVYEBoFDgSjcQ OK\n",
      "no more videos\n",
      "UCL_A4jkwvKuMyToAPy3FQKQ vidoes OK\n",
      "29 UCL_A4jkwvKuMyToAPy3FQKQ OK\n",
      "no more videos\n",
      "30 UCsXVk37bltHxD1rDPwtNM8Q OK\n",
      "UCY7dD6waquGnKTZSumPMTlQ vidoes OK\n",
      "31 UCY7dD6waquGnKTZSumPMTlQ OK\n",
      "no more videos\n",
      "32 UCdnZdQxYXnbN4uWJg96oGxw OK\n",
      "no more videos\n",
      "33 UCL_f53ZEJxp8TtlOkHwMV9Q OK\n",
      "UC6o-wWU-v2ClFMwougmK7dA vidoes OK\n",
      "34 UC6o-wWU-v2ClFMwougmK7dA OK\n",
      "no more videos\n",
      "35 UC0uVZd8N7FfIZnPu0y7o95A OK\n",
      "no more videos\n",
      "36 UCxCfoSInadl-4i3F70zDt1A OK\n",
      "no more videos\n",
      "37 UCEAZeUIeJs0IjQiqTCdVSIg OK\n",
      "no more videos\n",
      "UCEBb1b_L6zDS3xTUrIALZOw vidoes OK\n",
      "38 UCEBb1b_L6zDS3xTUrIALZOw OK\n",
      "no more videos\n",
      "39 UCU9Z1eLsDa_meEfsmx-Ys0w OK\n"
     ]
    }
   ],
   "source": [
    "from youtube_crawler import youtubeCrawler\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('APIkey.json') as f:\n",
    "    key = json.load(f);\n",
    "    apikey = key['APIkey'];\n",
    "\n",
    "\n",
    "YC = youtubeCrawler(apikey)\n",
    "\n",
    "channel_df = pd.read_csv('learning_data/channel_labeled.csv')\n",
    "channel_df\n",
    "\n",
    "video_labeled_df = pd.DataFrame(columns=['video_id', 'difficulty', 'channel_id'])\n",
    "\n",
    "\n",
    "for idx, channelID in enumerate(channel_df['channel_id']):\n",
    "    # if idx != 6: continue\n",
    "    difficulty = channel_df['difficulty'][idx]\n",
    "    try:\n",
    "        new_videos = YC.get_recent_videos(channelID)\n",
    "        for video_id in new_videos:\n",
    "            video_labeled_df.loc[len(video_labeled_df)] = [video_id, difficulty, channelID]\n",
    "        \n",
    "    except:\n",
    "        print('something wrong')\n",
    "        break\n",
    "    print(f'{idx} {channelID} OK')\n",
    "    \n",
    "\n",
    "video_labeled_df.to_csv('learning_data/video_labeled.csv', sep=',', index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "# 분석기를 통해 비디오 스크립트 분석 \n",
    "\n",
    " punctuator 가 머신러닝을 통해 작동하므로 적절한 환경에서 실행해야 오래 걸리지 않음"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-11e0db29151a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manalyzeAll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/capstone/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mword_analyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscript_analyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScriptAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpunctuator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPunctuator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/capstone/word_analyzer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpos_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpos_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'J'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADJ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mpos_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'V'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVERB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mpos_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'R'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from core import analyzeAll\n",
    "\n",
    "import json\n",
    "\n",
    "no_caption_videos =[]\n",
    "train_data = pd.DataFrame(columns=['videoId','totalWords','totalUniqueWords','totalSentences','avgSyllPerSec','avgCEFRScore','avgWordCEFR','avgFreqCEFR','readability','avgSentenceLength','uncommonRatio','totalEasyRatio','totalMiddleRatio','totalHardRatio','wordEasyRatio','wordMiddleRatio','wordHardRatio','FreqEasyRatio','FreqMiddleRatio','FreqHardRatio','channelId'])\n",
    "\n",
    "video_labeled = pd.read_csv('learning_data/video_labeled.csv')\n",
    "for idx,video_id in enumerate(video_labeled['video_id']):\n",
    "    if idx > -1:\n",
    "        print(f'{idx} {video_id} start')\n",
    "        try:\n",
    "            result = analyzeAll(video_id)\n",
    "            no_script_result = json.loads(result)\n",
    "            del no_script_result['script']\n",
    "            train_data.loc[len(train_data)] = no_script_result\n",
    "            channel_id = video_labeled['channel_id'][idx]\n",
    "            print(channel_id)\n",
    "            train_data.loc[len(train_data)-1, 'channelId'] = channel_id\n",
    "        except NotImplementedError:\n",
    "            print(f'{idx} {video_id} no captions')\n",
    "            no_caption_videos.append(video_id)\n",
    "        if idx % 300 == 0:\n",
    "            train_data.to_csv('learning_data/video_train_upgrade.csv',sep=',',index=False)\n",
    "            print(\"####temp download#########\")\n",
    "        \n",
    "\n",
    "\n",
    "train_data.to_csv('learning_data/video_train_upgrade.csv',sep=',',index=False)\n",
    "print(no_caption_videos)"
   ]
  },
  {
   "source": [
    "# 난이도 라벨과 분석 결과 합치기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data0 = pd.read_csv('learning_data/video_train.csv', sep=',')\n",
    "data1 = pd.read_csv('learning_data/video_train_1.csv', sep=',')\n",
    "data2 = pd.read_csv('learning_data/video_train_2.csv', sep=',')\n",
    "data3 = pd.read_csv('learning_data/video_train_3.csv', sep=',')\n",
    "\n",
    "\n",
    "merge = pd.concat([data0,data1,data2,data3], ignore_index=True)\n",
    "\n",
    "label = pd.read_csv('learning_data/video_labeled.csv', sep=',')\n",
    "\n",
    "result = pd.merge(merge, label, how='left', left_on='videoId', right_on='video_id')\n",
    "# result.drop(['channel_id', 'video_id'], axis=1, inplace=True)\n",
    "result.to_csv('learning_data/capstone_train_data.csv', sep=',', index=False)\n"
   ]
  }
 ]
}